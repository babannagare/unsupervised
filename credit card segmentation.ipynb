{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3668f48",
   "metadata": {},
   "source": [
    "## Enhancing Customer Engagement: A Segmentation Approach for Credit Card Users with Model Deployment on Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7678cee",
   "metadata": {},
   "source": [
    "Clustering: The primary objective of clustering is to group similar data points together based on certain features or characteristics. Clusters are formed to maximize the intra-cluster similarity and minimize the inter-cluster similarity.\n",
    "Segmentation: Segmentation is more business-oriented and aims to identify groups of customers or market segments that share similar behaviors, needs, or characteristics. It is often used in marketing and customer analysis to tailor strategies for different segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73791c5",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef013f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px # is a high-level interface for creating various types of interactive plots with minimal code. \n",
    "import plotly.graph_objects as go # is a lower-level interface that offers more control and customization over the appearance and behavior of your plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d71e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece965d5",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649aa6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"customer_data_credit_card.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb9dec",
   "metadata": {},
   "source": [
    "### Explorarory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59708931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e20cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d159ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f015964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['PAYMENTS'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_credit_limit = df['CREDIT_LIMIT'].mode()[0]\n",
    "df['CREDIT_LIMIT'].fillna(mode_credit_limit, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['PAYMENTS'] == 0, 'MINIMUM_PAYMENTS'] = df.loc[df['PAYMENTS'] == 0, 'MINIMUM_PAYMENTS'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_payments = df['PAYMENTS'].min()\n",
    "df['MINIMUM_PAYMENTS'].fillna(min_payments, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797704b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6650fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5acd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd518e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['CUST_ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649eb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_numeric_type(df, column_name):\n",
    "    unique_values_count = len(df[column_name].unique())\n",
    "    if unique_values_count < 10:\n",
    "        return 'Discrete'\n",
    "    else:\n",
    "        return 'Continuous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_numeric_columns = []\n",
    "continuous_numeric_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype == 'float64' or df[column].dtype == 'int64':\n",
    "        column_type = identify_numeric_type(df, column)\n",
    "        if column_type == 'Discrete':\n",
    "            discrete_numeric_columns.append(column)\n",
    "        elif column_type == 'Continuous':\n",
    "            continuous_numeric_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f432873",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Discrete Numeric Columns:', discrete_numeric_columns)\n",
    "print('\\n')\n",
    "print('Continuous Numeric Columns:', continuous_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83189b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    print(i)\n",
    "    print(df[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    print(i)\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a41d7",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    print('Countplot for:', i)\n",
    "    plt.figure(figsize=(15,6))\n",
    "    sns.countplot(x = df[i], data = df, palette = 'hls')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    print('Pie plot for:', i)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    df[i].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Distribution of ' + i)\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea92b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    fig = go.Figure(data=[go.Bar(x=df[i].value_counts().index, \n",
    "                                 y=df[i].value_counts())])\n",
    "    fig.update_layout(\n",
    "        title=i,\n",
    "        xaxis_title=i,\n",
    "        yaxis_title=\"Count\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679eb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    print('Pie plot for:', i)\n",
    "    fig = px.pie(df, names=i)\n",
    "    fig.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    plt.figure(figsize=(15,6))\n",
    "    sns.histplot(df[i], kde = True, bins = 20, palette = 'hls')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ff405",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    plt.figure(figsize=(15,6))\n",
    "    sns.distplot(df[i], kde = True, bins = 20)\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a013e32",
   "metadata": {},
   "source": [
    "In statistics and data visualization, KDE (Kernel Density Estimation) and PDF (Probability Density Function) are related concepts used to estimate and visualize the probability density of a continuous random variable.\n",
    "\n",
    "Kernel Density Estimation (KDE): Kernel Density Estimation is a non-parametric way to estimate the probability density function (PDF) of a random variable. It provides a smoother representation of the underlying probability distribution compared to traditional histograms. In KDE, a kernel (a smooth, symmetric, and usually bell-shaped function) is placed at each data point, and the overall density is obtained by summing these kernels. The bandwidth parameter controls the smoothness of the resulting density estimation.\n",
    "\n",
    "Probability Density Function (PDF): The Probability Density Function (PDF) is a function that describes the likelihood of a continuous random variable taking a particular value. For continuous random variables, the probability of the variable falling within a specific range is given by the integral of the PDF over that range. The PDF should satisfy two conditions: it should be non-negative for all values, and the total area under the curve (over all possible values) should equal 1.\n",
    "\n",
    "In summary, KDE is a method to estimate the PDF of a continuous random variable by smoothing the distribution using kernels, and the PDF is a mathematical function that describes the probability distribution of a continuous random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.boxplot(df[i],palette='hls')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.violinplot(df[i], palette='hls')\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75cad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    fig = go.Figure(data=[go.Histogram(x=df[i])])\n",
    "    fig.update_layout(\n",
    "        title=i,\n",
    "        xaxis_title=i,\n",
    "        yaxis_title=\"Value\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c5a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    fig = go.Figure(data=[go.Box(x=df[i])])\n",
    "    fig.update_layout(\n",
    "        title=i,\n",
    "        xaxis_title=i,\n",
    "        yaxis_title=\"Value\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    fig = go.Figure(data=[go.Violin(x=df[i])])\n",
    "    fig.update_layout(\n",
    "        title=i,\n",
    "        xaxis_title=i,\n",
    "        yaxis_title=\"Value\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    for j in continuous_numeric_columns:\n",
    "        plt.figure(figsize=(15,6))\n",
    "        sns.barplot(x = df[i], y = df[j], data = df, ci = None, palette = 'hls')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    for j in continuous_numeric_columns:\n",
    "        plt.figure(figsize=(15,6))\n",
    "        sns.boxplot(x = df[i], y = df[j], data = df, palette = 'hls')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08545a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    for j in continuous_numeric_columns:\n",
    "        plt.figure(figsize=(15,6))\n",
    "        sns.violinplot(x = df[i], y = df[j], data = df, palette = 'hls')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in discrete_numeric_columns:\n",
    "#     for j in continuous_numeric_columns:\n",
    "#         fig = go.Figure()\n",
    "#         fig.add_trace(go.Bar(x=df[i], y=df[j], name=f'{i} vs {j}'))\n",
    "#         fig.update_layout(title=f'{i} vs {j}', xaxis_title=i, yaxis_title=j)\n",
    "#         fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in discrete_numeric_columns:\n",
    "#     for j in continuous_numeric_columns:\n",
    "#         fig = go.Figure()\n",
    "#         fig.add_trace(go.Box(x=df[i], y=df[j], name=f'{i} vs {j}'))\n",
    "#         fig.update_layout(title=f'{i} vs {j}', xaxis_title=i, yaxis_title=j)\n",
    "#         fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in discrete_numeric_columns:\n",
    "    for j in continuous_numeric_columns:\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Violin(x=df[i], y=df[j], name=f'{i} vs {j}'))\n",
    "        fig.update_layout(title=f'{i} vs {j}', xaxis_title=i, yaxis_title=j)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    for j in continuous_numeric_columns:\n",
    "        if i != j:\n",
    "            plt.figure(figsize=(15,6))\n",
    "            sns.lineplot(x = df[j], y = df[i], data = df, palette = 'hls')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    for j in continuous_numeric_columns:\n",
    "        if i != j:\n",
    "            plt.figure(figsize=(15,6))\n",
    "            sns.scatterplot(x = df[j], y = df[i], data = df, palette = 'hls')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in continuous_numeric_columns:\n",
    "    for j in continuous_numeric_columns:\n",
    "        if i != j:\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=df[j], y=df[i], mode='markers', \n",
    "                                     marker=dict(color='blue', opacity=0.6),\n",
    "                                     name=f'{i} vs {j}'))\n",
    "            fig.update_layout(title=f'{i} vs {j}', xaxis_title=j, yaxis_title=i)\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ef7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Utilization Analysis\n",
    "df['CREDIT_UTILIZATION'] = df['PURCHASES'] / df['CREDIT_LIMIT']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='CREDIT_UTILIZATION', y='MINIMUM_PAYMENTS', data=df)\n",
    "plt.title('Credit Utilization vs. Minimum Payments')\n",
    "plt.xlabel('Credit Utilization')\n",
    "plt.ylabel('Minimum Payments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5794fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavioral Patterns Analysis\n",
    "behavioral_features = ['PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES']\n",
    "df[behavioral_features].plot(kind='hist', bins=30, alpha=0.7, figsize=(12, 6))\n",
    "plt.title('Purchase Patterns')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d30c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, \n",
    "                   x=behavioral_features, \n",
    "                   nbins=30, \n",
    "                   opacity=0.7, \n",
    "                   labels={'value': 'Value', 'count': 'Count'},\n",
    "                   title='Purchase Patterns')\n",
    "\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd967a",
   "metadata": {},
   "source": [
    "### removing outliers based on IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df1.quantile(0.10)                   ## selcting loweer 10 % qunatitle\n",
    "Q3 = df1.quantile(0.90)                   #  selecting higher 90 %\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df2= df1[((df1 >= lower_bound) & (df1 <= upper_bound)).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e165e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ee48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c01fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9059539",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(1, len(explained_variance_ratio) + 1),\n",
    "                         y=np.cumsum(explained_variance_ratio),\n",
    "                         mode='lines+markers',\n",
    "                         name='Cumulative Explained Variance'))\n",
    "fig.add_trace(go.Bar(x=np.arange(1, len(explained_variance_ratio) + 1),\n",
    "                     y=explained_variance_ratio,\n",
    "                     name='Explained Variance Ratio'))\n",
    "\n",
    "fig.update_layout(title='Explained Variance Ratio by Principal Component',\n",
    "                  xaxis_title='Principal Component',\n",
    "                  yaxis_title='Explained Variance Ratio',\n",
    "                  showlegend=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de17847",
   "metadata": {},
   "source": [
    "\"Explained Variance\" and \"Cumulative Explained Variance\" are essential concepts, especially when dealing with dimensionality reduction techniques such as Principal Component Analysis (PCA). Let's break down these terms:\n",
    "\n",
    "### Explained Variance (EV) or Explained Variance Ratio (EVR):\n",
    "\n",
    "- **Explained Variance (EV):** In the context of PCA, the explained variance is the amount of variance captured by each principal component. It tells us how much information (variance) is attributed to each of the principal components.\n",
    "\n",
    "- **Explained Variance Ratio (EVR):** It is the proportion of the dataset's variance that lies along the axis of each principal component. For each principal component, the explained variance ratio is the ratio of the variance captured by that component to the total variance.\n",
    "\n",
    "In mathematical terms, for a particular principal component \\(i\\), the explained variance ratio \\(EV_i\\) is calculated as:\n",
    "\n",
    "\\[ EV_i = \\frac{\\text{Variance along PC}_i}{\\text{Total Variance}} \\]\n",
    "\n",
    "The cumulative explained variance at \\(i\\) principal components is the sum of explained variance ratios up to the \\(i\\)th component. It represents the total amount of variance captured by considering the first \\(i\\) principal components.\n",
    "\n",
    "### Cumulative Explained Variance:\n",
    "\n",
    "- **Cumulative Explained Variance:** This is the accumulated amount of variance explained by the first \\(i\\) principal components. It helps us understand how much of the total variance in the dataset is explained by including more principal components.\n",
    "\n",
    "In mathematical terms, for \\(i\\) principal components, the cumulative explained variance \\(CEV_i\\) is calculated as:\n",
    "\n",
    "\\[ CEV_i = \\sum_{k=1}^{i} EV_k \\]\n",
    "\n",
    "In a practical sense, it's common to plot the cumulative explained variance against the number of principal components. This plot helps in deciding how many principal components to retain for dimensionality reduction while capturing a significant portion of the dataset's variance.\n",
    "\n",
    "The cumulative explained variance plot often shows an \"elbow\" point, which can help determine the optimal number of principal components to retain based on the diminishing returns of additional components in explaining the variance.\n",
    "\n",
    "Understanding these metrics is crucial for making informed decisions about the number of principal components to keep in dimensionality reduction techniques like PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf947b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98765691",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names = [x for _, x in sorted(zip(explained_variance_ratio, feature_names), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef048c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature in enumerate(sorted_feature_names, start=1):\n",
    "    print(f\"{i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_features = go.Figure(go.Bar(\n",
    "    x=feature_names,\n",
    "    y=pca.explained_variance_ratio_,\n",
    "    text=feature_names,\n",
    "    hoverinfo='x+y',\n",
    "))\n",
    "fig_features.update_layout(\n",
    "    title='Feature Contributions to Explained Variance',\n",
    "    xaxis_title='Features',\n",
    "    yaxis_title='Explained Variance Ratio',\n",
    ")\n",
    "fig_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.5\n",
    "\n",
    "correlation_matrix = df2.corr().abs()\n",
    "\n",
    "high_correlation_features = []\n",
    "for feature in df2.columns:\n",
    "    correlated_features = correlation_matrix[feature][correlation_matrix[feature] > correlation_threshold].index.tolist()\n",
    "    correlated_features.remove(feature)  # Remove self-correlation\n",
    "    if correlated_features:\n",
    "        high_correlation_features.append((feature, correlated_features))\n",
    "\n",
    "for feature, correlated_features in high_correlation_features:\n",
    "    print(f\"Feature '{feature}' is highly correlated with: {', '.join(correlated_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912b62f",
   "metadata": {},
   "source": [
    "Certainly! Here's a summary of the high correlations observed among the features:\n",
    "\n",
    "1. **BALANCE** is strongly correlated with:\n",
    "   - **CASH_ADVANCE**: Reflecting a link between cash advances and account balance.\n",
    "   - **MINIMUM_PAYMENTS**: Suggesting a relationship between minimum payments and account balance.\n",
    "\n",
    "2. **PURCHASES** is highly correlated with:\n",
    "   - **ONEOFF_PURCHASES**, **INSTALLMENTS_PURCHASES**: Indicating different types of purchases are correlated with total purchases.\n",
    "   - **PURCHASES_FREQUENCY**, **ONEOFF_PURCHASES_FREQUENCY**: Suggesting the frequency of purchases correlates with total purchases.\n",
    "   - **PURCHASES_TRX**: Showing a correlation with the number of purchase transactions.\n",
    "\n",
    "3. **ONEOFF_PURCHASES** is highly correlated with:\n",
    "   - **PURCHASES**, **ONEOFF_PURCHASES_FREQUENCY**: Indicating a link with general purchases and frequency of significant purchases.\n",
    "   - **PURCHASES_TRX**: Showing a correlation with the number of purchase transactions.\n",
    "   - **CREDIT_UTILIZATION**: Suggesting a relationship with credit card utilization for one-off purchases.\n",
    "\n",
    "4. **INSTALLMENTS_PURCHASES** is highly correlated with:\n",
    "   - **PURCHASES_FREQUENCY**, **PURCHASES_INSTALLMENTS_FREQUENCY**: Suggesting a link with purchase frequency and installment purchases.\n",
    "   - **PURCHASES_TRX**: Showing a correlation with the number of purchase transactions.\n",
    "\n",
    "5. **CASH_ADVANCE** is highly correlated with:\n",
    "   - **BALANCE**: Reflecting a correlation with account balance.\n",
    "   - **CASH_ADVANCE_FREQUENCY**, **CASH_ADVANCE_TRX**: Indicating a relationship with the frequency and number of cash advances.\n",
    "\n",
    "6. **PURCHASES_FREQUENCY** is highly correlated with:\n",
    "   - **PURCHASES**, **INSTALLMENTS_PURCHASES**, **PURCHASES_INSTALLMENTS_FREQUENCY**: Showing a link with different aspects of purchase frequency.\n",
    "   - **PURCHASES_TRX**: Correlated with the number of purchase transactions.\n",
    "\n",
    "7. **ONEOFF_PURCHASES_FREQUENCY** is highly correlated with:\n",
    "   - **PURCHASES**, **ONEOFF_PURCHASES**, **PURCHASES_TRX**: Indicating a correlation with purchases and frequency of significant purchases.\n",
    "\n",
    "8. **PURCHASES_INSTALLMENTS_FREQUENCY** is highly correlated with:\n",
    "   - **INSTALLMENTS_PURCHASES**, **PURCHASES_FREQUENCY**: Suggesting a link with purchase frequency and installment purchases.\n",
    "   - **PURCHASES_TRX**: Showing a correlation with the number of purchase transactions.\n",
    "\n",
    "9. **CASH_ADVANCE_FREQUENCY** is highly correlated with:\n",
    "   - **CASH_ADVANCE**, **CASH_ADVANCE_TRX**: Indicating a relationship with the frequency and number of cash advances.\n",
    "\n",
    "10. **CASH_ADVANCE_TRX** is highly correlated with:\n",
    "    - **CASH_ADVANCE**, **CASH_ADVANCE_FREQUENCY**: Showing a correlation with cash advances and their frequency.\n",
    "\n",
    "11. **PURCHASES_TRX** is highly correlated with:\n",
    "    - **PURCHASES**, **ONEOFF_PURCHASES**, **INSTALLMENTS_PURCHASES**, **PURCHASES_FREQUENCY**, **ONEOFF_PURCHASES_FREQUENCY**, **PURCHASES_INSTALLMENTS_FREQUENCY**: Indicating a strong relationship with various transactional aspects.\n",
    "\n",
    "12. **MINIMUM_PAYMENTS** is highly correlated with:\n",
    "    - **BALANCE**: Showing a relationship between account balance and minimum payments.\n",
    "\n",
    "13. **CREDIT_UTILIZATION** is highly correlated with:\n",
    "    - **PURCHASES**, **ONEOFF_PURCHASES**, **INSTALLMENTS_PURCHASES**, **PURCHASES_FREQUENCY**, **PURCHASES_TRX**: Indicating a correlation with various aspects of credit card usage.\n",
    "\n",
    "These correlations provide valuable insights into the relationships among different features, which can be used for further analysis and segmentation in the credit card domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'BALANCE',\n",
    "    'PURCHASES',\n",
    "    'ONEOFF_PURCHASES',\n",
    "    'INSTALLMENTS_PURCHASES',\n",
    "    'CASH_ADVANCE',\n",
    "    'CREDIT_LIMIT',\n",
    "    'PAYMENTS',\n",
    "    'PRC_FULL_PAYMENT',\n",
    "    'TENURE',\n",
    "    'CREDIT_UTILIZATION'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5aaa56",
   "metadata": {},
   "source": [
    "selected_features = ['BALANCE', 'CASH_ADVANCE', 'MINIMUM_PAYMENTS', \n",
    "                     'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES',\n",
    "                     'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', \n",
    "                     'PURCHASES_TRX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ea456",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = df2[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767016b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff452f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(selected_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46467d95",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7069c",
   "metadata": {},
   "source": [
    "Inertia: Inertia, also known as within-cluster sum of squares, is a measure of how far the points within a cluster are from the center of that cluster (centroid). It quantifies the compactness of the clusters. The lower the inertia, the better the clustering.\n",
    "\n",
    "KMeans Clustering: KMeans is a popular clustering algorithm that partitions a dataset into 'k' distinct, non-overlapping subgroups or clusters. It aims to minimize the within-cluster variance (inertia) by iteratively reassigning data points to clusters and updating the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166377b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Elbow Method\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal Cluster Selection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abde040",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(range(1, 11)), y=inertia, mode='lines+markers'))\n",
    "fig.update_layout(title='Elbow Method for Optimal Cluster Selection',\n",
    "                  xaxis=dict(title='Number of Clusters'),\n",
    "                  yaxis=dict(title='Inertia'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e61804",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b308d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57638a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfa = pd.DataFrame(data = scaled_data, columns = selected_features)\n",
    "new_dfa['label_kmeans'] = clusters\n",
    "new_dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f63e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfa['label_kmeans'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cc41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print('Centroids:', centroids)\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data_frame=new_dfa, x='BALANCE', y='PURCHASES', color='label_kmeans',\n",
    "                 title='Customer Segmentation based on Balance and Purchases',\n",
    "                 labels={'BALANCE': 'Balance', 'PURCHASES': 'Purchases'},\n",
    "                 color_continuous_scale='Viridis')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=new_dfa['BALANCE'], y=new_dfa['PURCHASES'], mode='markers', \n",
    "                         marker=dict(color=new_dfa['label_kmeans'], colorscale='Viridis', opacity=0.6),\n",
    "                         text='Cluster ' + new_dfa['label_kmeans'].astype(str),\n",
    "                         hoverinfo='text'))\n",
    "fig.add_trace(go.Scatter(x=centroids[:, 0], y=centroids[:, 1], \n",
    "                         mode='markers', marker=dict(color='red', size=10, symbol='cross'),\n",
    "                         name='Cluster Centroids'))\n",
    "fig.update_layout(title='Customer Segmentation based on Balance and Purchases with Cluster Centroids',\n",
    "                  xaxis=dict(title='Balance'),\n",
    "                  yaxis=dict(title='Purchases'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb4112",
   "metadata": {},
   "source": [
    "Hierarchical clustering is a popular and powerful method used in data analysis and machine learning for the purpose of grouping or clustering similar data points together. It is a bottom-up approach to clustering, where data points are initially treated as individual clusters and are successively merged or grouped together based on their similarity. This merging process continues until all data points belong to a single, large cluster or until a predefined number of clusters is reached.\n",
    "\n",
    "Here's a step-by-step explanation of how hierarchical clustering works:\n",
    "\n",
    "1. **Initialization**: Start by considering each data point as an individual cluster. So, if you have N data points, you initially have N clusters.\n",
    "\n",
    "2. **Calculate Pairwise Distances**: Compute the pairwise distances or dissimilarities between all pairs of data points. The choice of distance metric (e.g., Euclidean distance, Manhattan distance, etc.) depends on the nature of the data and the problem.\n",
    "\n",
    "3. **Merge Closest Clusters**: Identify the two closest clusters based on the computed distances and merge them into a single cluster. This process continues until there is only one cluster left (agglomerative) or until you have the desired number of clusters (divisive).\n",
    "\n",
    "4. **Distance Metric**: Choose a linkage criterion to determine how the distance between clusters is calculated during the merging process. Common linkage methods include:\n",
    "   - **Single Linkage**: Merge clusters based on the minimum distance between any two data points in the clusters.\n",
    "   - **Complete Linkage**: Merge clusters based on the maximum distance between any two data points in the clusters.\n",
    "   - **Average Linkage**: Merge clusters based on the average distance between all pairs of data points in the clusters.\n",
    "   - **Ward's Linkage**: Minimize the increase in the total within-cluster variance when merging clusters.\n",
    "\n",
    "5. **Dendrogram**: As the clusters are merged, a dendrogram is typically created. A dendrogram is a tree-like structure that visually represents the merging process and allows you to see how the clusters are formed at different levels.\n",
    "\n",
    "6. **Choosing the Number of Clusters**: You can cut the dendrogram at a certain height or level to determine the number of clusters you want. This allows you to control the granularity of the clustering.\n",
    "\n",
    "Hierarchical clustering has several advantages, including its ability to reveal the hierarchical structure of the data and its flexibility in choosing the number of clusters. However, it can be computationally expensive for large datasets.\n",
    "\n",
    "In summary, hierarchical clustering is a versatile technique used for grouping data points into clusters based on their similarity or dissimilarity, and it provides valuable insights into the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ac4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0463f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = sch.distance.pdist(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = sch.linkage(distance_matrix, method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.title('Dendrogram')\n",
    "dendrogram = sch.dendrogram(linkage)\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5bfbc",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm widely used in machine learning and data analysis. Unlike k-means or hierarchical clustering, DBSCAN does not require the number of clusters to be specified in advance and can discover arbitrarily shaped clusters. It's particularly effective in scenarios where the clusters have irregular shapes and different densities.\n",
    "\n",
    "Here's a step-by-step explanation of how DBSCAN works:\n",
    "\n",
    "1. **Initialization**: Start with an arbitrary data point that has not been visited.\n",
    "\n",
    "2. **Density-Based Neighborhood Search**: For this point and its neighborhood, defined by a distance parameter ε (epsilon), find all nearby points. These are the density-connected points.\n",
    "\n",
    "3. **Density Connectivity**: If the number of density-connected points is greater than a predefined threshold (MinPts), consider this point and its neighborhood as a cluster. \n",
    "\n",
    "4. **Expand the Cluster**: Expand this cluster by recursively repeating the neighborhood search and density connectivity steps for all density-connected points.\n",
    "\n",
    "5. **Form Additional Clusters**: If a point is found to be a density-connected point to multiple clusters, it's considered a border point. If it's not density-connected to any cluster, it's treated as noise.\n",
    "\n",
    "DBSCAN classifies each point in the dataset into one of the following:\n",
    "\n",
    "- **Core Points**: These are the points that have at least MinPts within their ε-neighborhood. They start a new cluster or expand an existing one.\n",
    "  \n",
    "- **Border Points**: These have fewer than MinPts within the ε-neighborhood but are in the neighborhood of a core point. They belong to the cluster of that core point.\n",
    "\n",
    "- **Noise or Outliers**: These are points that are neither core points nor in the ε-neighborhood of a core point. They do not belong to any cluster.\n",
    "\n",
    "Key features and advantages of DBSCAN:\n",
    "\n",
    "- **Flexibility in Cluster Shape**: DBSCAN can find arbitrarily shaped clusters and is not sensitive to the order of the data.\n",
    "\n",
    "- **Robust to Noise**: Noise in the form of outliers does not affect the clustering process significantly.\n",
    "\n",
    "- **Automatic Cluster Number**: It does not require specifying the number of clusters beforehand, unlike k-means.\n",
    "\n",
    "- **Efficient**: It's relatively efficient and can handle large datasets.\n",
    "\n",
    "However, setting the distance parameter ε and the minimum number of points MinPts appropriately can be a challenge, and the results can be sensitive to these parameters. Overall, DBSCAN is a powerful tool for clustering data based on density and is widely used in various applications such as anomaly detection, spatial analysis, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea03ab1",
   "metadata": {},
   "source": [
    "DBSCAN clustering using scikit-learn's `DBSCAN` class with a specified epsilon (eps) value of 0.5 and a minimum number of samples (min_samples) of 5. Let me know if there's something specific you would like to discuss or if you have any questions related to this code or DBSCAN clustering in general!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe839d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54943244",
   "metadata": {},
   "source": [
    "df2['dbscan'] = dbscan_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5f56b",
   "metadata": {},
   "source": [
    "Mean Shift is a clustering algorithm used in unsupervised machine learning to identify clusters or groupings in a dataset based on its underlying probability density function. Unlike K-means, Mean Shift doesn't require the number of clusters to be predefined.\n",
    "\n",
    "Here's a step-by-step explanation of the Mean Shift algorithm:\n",
    "\n",
    "1. **Initialization**: Initialize a set of data points as the starting points for the algorithm.\n",
    "\n",
    "2. **Kernel Estimation**: For each data point in the dataset, estimate its probability density function using a kernel function (e.g., Gaussian kernel). The kernel function assigns weights to the neighboring points, with closer points receiving higher weights.\n",
    "\n",
    "3. **Mean Shift**: For each data point, calculate the mean shift vector, which represents the direction and magnitude to shift the data point for maximizing the local density of points. This is done by computing the weighted mean of the data points based on the kernel.\n",
    "\n",
    "4. **Update Data Points**: Shift each data point in the direction of the mean shift vector.\n",
    "\n",
    "5. **Convergence**: Repeat steps 2-4 until the data points converge to stable positions. Convergence occurs when the mean shift vector becomes very small.\n",
    "\n",
    "6. **Cluster Assignment**: Group data points that converge to the same stable position into a cluster.\n",
    "\n",
    "Mean Shift tends to find a variable number of clusters based on the density of the data. It's particularly useful for applications where the number of clusters is not known a priori, and it can identify arbitrarily shaped clusters.\n",
    "\n",
    "The key parameters in Mean Shift are the bandwidth or radius of the kernel (which influences the size of clusters) and the choice of the kernel function. Adjusting the bandwidth can have a significant impact on the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72030ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "meanshift = MeanShift()\n",
    "meanshift_labels = meanshift.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanshift_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea9b07",
   "metadata": {},
   "source": [
    "Gaussian Mixture Model (GMM) is a probabilistic model used for clustering and density estimation. It represents a mixture of several Gaussian distributions, each characterized by its mean and covariance matrix.\n",
    "\n",
    "Here's a step-by-step explanation of how Gaussian Mixture Model works:\n",
    "\n",
    "1. **Initialization**: Start by initializing the parameters of the model. This includes selecting the number of Gaussian components (clusters) and their initial mean, covariance, and weights.\n",
    "\n",
    "2. **Expectation-Maximization (EM) Algorithm**:\n",
    "   - **Expectation (E-step)**: Calculate the probability that each data point belongs to each Gaussian component. This step computes the posterior probability using Bayes' rule.\n",
    "   - **Maximization (M-step)**: Update the parameters (mean, covariance, and weight) of each Gaussian component to maximize the likelihood of the observed data based on the probabilities calculated in the E-step.\n",
    "\n",
    "3. **Convergence Check**: Check for convergence by evaluating the change in log-likelihood or the change in parameters. If the change is below a certain threshold, the algorithm has converged.\n",
    "\n",
    "4. **Repeat Steps 2 and 3**: Continue iterating between the E-step and M-step until the algorithm converges.\n",
    "\n",
    "5. **Cluster Assignment**: After convergence, each data point is assigned to the Gaussian component with the highest probability.\n",
    "\n",
    "Gaussian Mixture Model is very flexible and can model a wide range of data distributions. It's capable of fitting complex data patterns and can identify clusters with different shapes and sizes. Additionally, GMM provides a probabilistic framework, meaning it assigns probabilities to each point's membership in each cluster rather than a hard assignment.\n",
    "\n",
    "The number of Gaussian components (clusters) in GMM needs to be specified a priori or determined using model selection techniques like the Bayesian Information Criterion (BIC) or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6db88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3)  # specify the number of components (clusters)\n",
    "gmm_labels = gmm.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad21b6",
   "metadata": {},
   "source": [
    "Agglomerative Clustering is a hierarchical clustering technique that builds a dendrogram (tree-like diagram) to illustrate the arrangement of the clusters. It's a \"bottom-up\" clustering method, where each data point starts as its own cluster and pairs of clusters are merged based on a specific criterion until a single cluster containing all data points is formed.\n",
    "\n",
    "Here's a step-by-step explanation of how Agglomerative Clustering works:\n",
    "\n",
    "1. **Initialization**: Start by considering each data point as an individual cluster. The number of initial clusters is equal to the number of data points.\n",
    "\n",
    "2. **Distance Calculation**: Compute the pairwise distance (e.g., Euclidean distance) between all clusters.\n",
    "\n",
    "3. **Merge Clusters**: Identify the two clusters that are closest to each other based on the chosen distance metric. Merge these clusters into a single cluster.\n",
    "\n",
    "4. **Update Distance Matrix**: Recalculate the distances between the new cluster and all other remaining clusters.\n",
    "\n",
    "5. **Repeat Steps 3 and 4**: Repeat the process of merging the closest clusters and updating the distance matrix until only a single cluster remains.\n",
    "\n",
    "6. **Dendrogram Construction**: Construct a dendrogram to illustrate the clustering process, showing the merging of clusters at different distances.\n",
    "\n",
    "7. **Choosing the Number of Clusters**: Determine the number of clusters by selecting a distance threshold or cutting the dendrogram at a certain height, which corresponds to the desired number of clusters.\n",
    "\n",
    "Agglomerative Clustering offers flexibility in determining the number of clusters after observing the dendrogram. It also allows for different linkage criteria, such as Ward's linkage, complete linkage, average linkage, and single linkage, which affect how the distance between clusters is calculated during the merging process.\n",
    "\n",
    "One drawback is its time complexity, which can be \\(O(n^3)\\) for a complete dataset, making it less efficient for large datasets. However, with various optimization techniques, this complexity can be reduced to \\(O(n^2 \\log n)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "agg_labels = agg_clustering.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382386b5",
   "metadata": {},
   "source": [
    "OPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that extends the concepts of DBSCAN (Density-Based Spatial Clustering of Applications with Noise). It aims to overcome some limitations of DBSCAN, particularly in handling clusters of varying densities and identifying noise in the data.\n",
    "\n",
    "Here's a breakdown of how OPTICS works:\n",
    "\n",
    "1. **Reachability Distance**: Similar to DBSCAN, OPTICS defines a reachability distance for each point. The reachability distance of point A from point B is defined as the maximum of the distance between A and B and the \\( \\epsilon \\) parameter.\n",
    "\n",
    "2. **Core Distance**: For each point, compute its core distance, which is the distance to the \\( \\text{minPts} \\)th nearest neighbor (where \\( \\text{minPts} \\) is a parameter specified by the user).\n",
    "\n",
    "3. **Ordering Points**: Sort the points based on their core distances in ascending order. This ordering helps in identifying clusters and noise.\n",
    "\n",
    "4. **Building Reachability Plot**: Create a reachability plot, which shows the reachability distances of each point from its \\( \\text{minPts} \\)th nearest neighbor. It helps identify regions of varying densities.\n",
    "\n",
    "5. **Clustering**: Based on the reachability plot, clusters are identified as regions where the reachability distance is within a specified range. Clusters can have varying densities.\n",
    "\n",
    "6. **Extracting Clusters**: Extract the clusters from the reachability plot. A cluster is formed by connecting the points where the reachability distance exceeds a specified threshold.\n",
    "\n",
    "OPTICS provides a more flexible approach to clustering by allowing the detection of clusters with different densities and handling noise effectively. It also offers the advantage of not requiring the user to specify the number of clusters in advance, making it suitable for a variety of datasets. However, it may be computationally expensive for large datasets, particularly due to the need to compute the reachability distances for all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3224a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "optics = OPTICS(eps=0.5, min_samples=5)\n",
    "optics_labels = optics.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e6ba2",
   "metadata": {},
   "source": [
    "# Cluster Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1335ef7",
   "metadata": {},
   "source": [
    "Cluster profiling, in the context of clustering algorithms, involves analyzing and summarizing the characteristics of clusters that have been identified in a dataset. It's an important step in understanding the nature and properties of different clusters generated by the clustering algorithm.\n",
    "\n",
    "Here are the main steps involved in cluster profiling:\n",
    "\n",
    "1. **Cluster Description**: Summarize the main features and properties of each cluster. This includes statistical measures like mean, median, standard deviation, etc., for numerical features, and mode for categorical features.\n",
    "\n",
    "2. **Visualization**: Create visual representations of the clusters to better understand their characteristics. Common visualizations include scatter plots for 2D or 3D feature spaces, box plots, parallel coordinate plots, and cluster center visualizations.\n",
    "\n",
    "3. **Feature Importance**: Determine the importance of different features in defining each cluster. This can be done using various techniques such as analyzing feature importance scores from the clustering algorithm or using domain-specific knowledge.\n",
    "\n",
    "4. **Comparison between Clusters**: Compare the characteristics of different clusters to identify patterns or anomalies. This helps in understanding the distinctive traits of each cluster.\n",
    "\n",
    "5. **Segment Description**: Provide a clear and interpretable description of each segment or cluster. This often involves summarizing the common traits, behaviors, or characteristics of the data points within a cluster.\n",
    "\n",
    "6. **Business Insights**: Translate the technical understanding of the clusters into actionable business insights. Understanding the characteristics of each cluster helps in tailoring strategies, marketing campaigns, or product offerings to specific customer segments.\n",
    "\n",
    "Cluster profiling is crucial in various domains such as customer segmentation, marketing, healthcare, finance, and more. It helps in making informed decisions based on the knowledge extracted from the clusters, ultimately driving better business outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31912e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_original = scaler.inverse_transform(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18896342",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_df = pd.DataFrame(cluster_centers_original, columns=selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a82cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_clusters):\n",
    "    plt.bar(selected_features, cluster_centers_df.iloc[i], alpha=0.5, label=f'Cluster {i}')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.title('Mean Values of Features by Cluster')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cluster_centers_df.transpose()  \n",
    "fig = px.bar(data, \n",
    "             x=data.index, \n",
    "             y=data.columns, \n",
    "             barmode='group',\n",
    "             title='Mean Values of Features by Cluster',\n",
    "             labels={'x': 'Features', 'y': 'Mean Value'},\n",
    "             category_orders={\"x\": selected_features},\n",
    "             width=800, height=400)\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clusters):\n",
    "    print(f\"\\nCluster {i} Characteristics:\")\n",
    "    for feature, centroid_value in zip(selected_features, cluster_centers_df.iloc[i]):\n",
    "        print(f\"- {feature}: {centroid_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c67e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_descriptions = [\n",
    "    \"Segment 0: These customers have a moderate balance, make regular purchases, and use credit moderately.\",\n",
    "    \"Segment 1: These customers have a high balance, make frequent purchases, and utilize credit extensively.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSegment Descriptions:\")\n",
    "for description in segment_descriptions:\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_recommendations = {\n",
    "    0: \"Recommendation for Segment 0: Encourage customers to make more regular purchases to maximize their credit benefits. Offer personalized product suggestions based on their purchase history.\",\n",
    "    1: \"Recommendation for Segment 1: Leverage targeted marketing campaigns to promote high-end products or credit limit upgrades, considering their high credit utilization and purchase frequency.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tailored Recommendations:\")\n",
    "for segment, recommendation in segment_recommendations.items():\n",
    "    print(f\"\\nSegment {segment} Recommendations:\")\n",
    "    print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "kmeans_model_file = \"kmeans_model.pkl\"\n",
    "\n",
    "with open(kmeans_model_file, \"wb\") as file:\n",
    "    pickle.dump(kmeans, file)\n",
    "\n",
    "print(\"K-means model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_version = st.__version__\n",
    "print(\"Streamlit version:\", streamlit_version)\n",
    "\n",
    "pandas_version = pd.__version__\n",
    "print(\"Pandas version:\", pandas_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn_version = sklearn.__version__\n",
    "print(\"scikit-learn version:\", sklearn_version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
